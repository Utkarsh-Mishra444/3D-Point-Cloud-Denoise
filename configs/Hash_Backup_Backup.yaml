data:
  data_dir: data/objects/
  dataset: PUNet
  augment: true
  use_rgb_features: false
  workers: 4
  npoints: 2048
diffusion:
  timesteps: 1000
  sampling_timesteps: 10
  objective: pred_noise
  schedule: linear
  sampling_strategy: DDPM
  loss_type: mse
  beta_start: 1.e-4
  beta_end: 0.02
  t0: 1.e-4
  T: 1.0
  ot_ode: true
model:
  type: PVD
  ema: true
  in_dim: 3
  extra_feature_channels: 0
  out_dim: 3
  time_embed_dim: 64
  dropout: 0.15
  EMA:
    decay: 0.999
  PVD:
    use_global_embedding: false  # Set to false to simplify; set to true if you require it
    global_embedding_dim: 1024    # Relevant only if use_global_embedding is true
    feat_embed_dim: 64            # Adjusted to match transformer input dim
    transformer_heads: 16          # Number of attention heads in the transformer
    transformer_layers: 8         # Number of transformer encoder layers
    transformer_ffn_dim: 256      # Feedforward network dimension in transformer
    out_mlp: 128
training:
  optimizer:
    type: AdamW
    lr: 3.e-4
    beta1: 0.9
    beta2: 0.999
    weight_decay: 1.0e-05
  scheduler:
    type: constant
    lr_gamma: 0.999
  grad_clip:
    enabled: true
    value: 1.0
  bs: 32
  overfit: false
  amp: true
  steps: 200_000
  accumulation_steps: 1
  log_interval: 10
  save_interval: 20000
  viz_interval: 10000
  seed: 42
sampling:
  bs: 32
  num_iter: 8
